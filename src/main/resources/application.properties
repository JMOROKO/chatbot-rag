spring.application.name=chatbot-rag
# télécharger, installer et demarrer ollama : https://ollama.com/
# une fois installer : $ollama run (model). Ex : $ollama run llama3
spring.ai.ollama.chat.model=llama3
#Ollama expose une api rest sur localhost:11434 si le modèle ollama est installé sur une autre machine sur le serveur mettre l'adresse ip à la place du localhost
spring.ai.ollama.base-url=http://localhost:11434
server.port=8080
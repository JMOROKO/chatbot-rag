spring.application.name=chatbot-rag
#utilisation de ollama
# télécharger, installer et demarrer ollama : https://ollama.com/
# une fois installer : $ollama run (model). Ex : $ollama run llama3
#spring.ai.ollama.chat.model=llama3
#Ollama expose une api rest sur localhost:11434 si le modèle ollama est installé sur une autre machine sur le serveur mettre l'adresse ip à la place du localhost
#spring.ai.ollama.base-url=http://localhost:11434

#utilisation d'openai
spring.ai.openai.api-key=sk-proj-5WxKlBTEq8kSfXqsnnY7T3BlbkFJ4nAfhf9CtEUtn6Vmvl5w
spring.ai.openai.chat.options.model=gpt-4o


server.port=8080

#il demarre docker en meme temps que l'application
spring.docker.compose.enabled= false
# il demarre docker et l'arrête
#spring.docker.compose.lifecycle-management= start_only

spring.datasource.url=jdbc:postgresql://localhost:5433/cv-store
spring.datasource.username=admin
spring.datasource.password=password